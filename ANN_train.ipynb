{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca3663b3",
   "metadata": {},
   "source": [
    "## Parameters that may be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b890d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "activ_fun_list = ['relu'] # A list of strings representing the activation functions used during cross-validation.\n",
    "                          # Check the class SequenceMLP to see what functions are available\n",
    "lstm_size = 75 # Size of the LSTM layer. Set to 0 to use only an MLP\n",
    "weight_hyperparam = [1, 90] # The weight used in the loss function for positive-class predictions. Should be [1, number > 1]\n",
    "data_version = 'v5' # The version of the data to be used. Should be of the form v#. Should be left as v5\n",
    "window_size = 10 # The number of AAs before and after the central S/T. Used only when data_version in {'v3', 'v5'}\n",
    "batch_size = 32 # The batch size used during cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae8a6c0",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d5c783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General & data manipulation imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import mkdir\n",
    "from os.path import isdir, isfile\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn import preprocessing\n",
    "# Torch & model creation imports\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import OrderedDict\n",
    "# Training & validation imports\n",
    "from itertools import product\n",
    "# Results & visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "# Convenience imports\n",
    "import pdb\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd7959b",
   "metadata": {},
   "source": [
    "## Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1520f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_version in {'v1', 'v2'}:\n",
    "    window_size_path = ''\n",
    "    myshape_X = data.shape[1] - 1 # For convenience when declaring ANNs\n",
    "elif data_version in {'v3', 'v4'}:\n",
    "    window_size_path = f'_{window_size}-window'\n",
    "    myshape_X = 76 # Manually declaring, 76 because the v1 dataset had 76 features\n",
    "else:\n",
    "    window_size_path = f'_{window_size}-window'\n",
    "    myshape_X = 75 # Rounded the 76 to 75\n",
    "\n",
    "# Loading and transforming the data if using an LSTM\n",
    "if lstm_size:\n",
    "    lstm_data = torch.Tensor(np.load(f'OH_LSTM_data_{data_version}{window_size_path}.npy'))\n",
    "data = torch.Tensor(pd.read_csv(f'OH_data_{data_version}{window_size_path}.csv').values) # Used for all ANN types\n",
    "\n",
    "# Pre-declaring paths for convenience (to save / load results)\n",
    "if lstm_size:\n",
    "    working_dir = f'RNN_{lstm_size}_results_{data_version}-data{window_size_path}'\n",
    "else:\n",
    "    working_dir = f'ANN_results_{data_version}-data'\n",
    "if not isdir(working_dir):\n",
    "    mkdir(working_dir)\n",
    "\n",
    "# Setting each activ_fun to lowercase for consistency\n",
    "activ_fun_list = [activ_fun.casefold() for activ_fun in activ_fun_list]\n",
    "\n",
    "# Data splitting - 80% Cross Validation, 20% Test\n",
    "if lstm_size:\n",
    "    cv_data, test_data, cv_lstm_data, test_lstm_data = train_test_split(data, lstm_data, test_size = 0.2, random_state = 123)\n",
    "else:\n",
    "    cv_data, test_data = train_test_split(data, test_size = 0.2, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e641d76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, lstm_data = None):\n",
    "        self.Xdata = data[:, :-1]\n",
    "        self.ydata = data[:, -1].type(torch.LongTensor)\n",
    "        self.lstm_data = lstm_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.Xdata)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(self.lstm_data, torch.Tensor):\n",
    "            return self.Xdata[idx], self.ydata[idx], self.lstm_data[idx]\n",
    "        else:\n",
    "            return self.Xdata[idx], self.ydata[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f7142a",
   "metadata": {},
   "source": [
    "## Model & Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763718b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP or LSTM+MLP model\n",
    "class SequenceMLP(torch.nn.Module):\n",
    "    def __init__(self, layers, activ_fun = 'relu', lstm_size = 0):\n",
    "        super(SequenceMLP, self).__init__()\n",
    "        # Setup to convert string (first notebook cell) to activation function\n",
    "        if activ_fun == 'relu':\n",
    "            torch_activ_fun = torch.nn.ReLU()\n",
    "        elif activ_fun == 'tanh':\n",
    "            torch_activ_fun = torch.nn.Tanh()\n",
    "        elif activ_fun == 'sigmoid':\n",
    "            torch_activ_fun = torch.nn.Sigmoid()\n",
    "        elif activ_fun == 'tanhshrink':\n",
    "            torch_activ_fun = torch.nn.Tanhshrink()\n",
    "        elif activ_fun == 'selu':\n",
    "            torch_activ_fun = torch.nn.SELU()\n",
    "        #elif activ_fun == 'attention':\n",
    "        #    torch_activ_fun = torch.nn.MultiheadAttention(myshape_X, 4)\n",
    "        else:\n",
    "            raise ValueError(f'Invalid activ_fun. You passed {activ_fun}')\n",
    "\n",
    "        # LSTM cell\n",
    "        if lstm_size:\n",
    "            self.lstm = torch.nn.LSTM(20, lstm_size, num_layers=1, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Transforming layers list into OrderedDict with layers + activation\n",
    "        mylist = list()\n",
    "        for idx, elem in enumerate(layers):\n",
    "            mylist.append((f'Linear{idx}', torch.nn.Linear(layers[idx][0], layers[idx][1]) ))\n",
    "            if idx < len(layers)-1:\n",
    "                mylist.append((f'{activ_fun}{idx}', torch_activ_fun))\n",
    "        # OrderedDict into NN\n",
    "        self.model = torch.nn.Sequential(OrderedDict(mylist))\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, lstm_data = None):\n",
    "        if 'lstm' in dir(self):\n",
    "            _, (ht, _) = self.lstm(lstm_data) # Passing only the seq data through the LSTM\n",
    "            to_MLP = ht[-1]\n",
    "            out = self.model(to_MLP)\n",
    "        else:\n",
    "            out = self.model(x)\n",
    "        probs = self.sigmoid(out)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b914347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function that is called every epoch of training or validation\n",
    "def loop_model(model, optimizer, loader, epoch, evaluation = False):\n",
    "    if evaluation:\n",
    "        model.eval()\n",
    "    else:\n",
    "        model.train()\n",
    "    batch_losses = []\n",
    "    \n",
    "    for data in loader:\n",
    "        if lstm_size:\n",
    "            X, y, lstm = data\n",
    "            lstm = lstm.cuda()\n",
    "        else:\n",
    "            X, y = data\n",
    "            lstm = None\n",
    "        X = X.cuda()\n",
    "        y = y.cuda()\n",
    "        pred = model(X, lstm)\n",
    "        loss = my_loss(pred, y)\n",
    "        # Backpropagation\n",
    "        if not evaluation:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # Saving losses & displaying them\n",
    "        batch_losses.append(loss.item())\n",
    "    \n",
    "    return np.array(batch_losses).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e71a1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the hyperparameters\n",
    "layers = [\n",
    "    # 1 hidden layer\n",
    "    #[(myshape_X, myshape_X*12), (myshape_X*12, 2)],\n",
    "    #[(myshape_X, myshape_X*11), (myshape_X*11, 2)],\n",
    "    #[(myshape_X, myshape_X*10), (myshape_X*10, 2)],\n",
    "    [(myshape_X, myshape_X*9), (myshape_X*9, 2)],\n",
    "    [(myshape_X, myshape_X*8), (myshape_X*8, 2)],\n",
    "    [(myshape_X, myshape_X*7), (myshape_X*7, 2)],\n",
    "    [(myshape_X, myshape_X*6), (myshape_X*6, 2)],\n",
    "    [(myshape_X, myshape_X*5), (myshape_X*5, 2)],\n",
    "    #[(myshape_X, myshape_X*4), (myshape_X*4, 2)],\n",
    "    #[(myshape_X, myshape_X*3), (myshape_X*3, 2)],\n",
    "    #[(myshape_X, myshape_X*2), (myshape_X*2, 2)],\n",
    "    #[(myshape_X, myshape_X), (myshape_X, 2)],\n",
    "    # 2 hidden layers\n",
    "    #[(myshape_X, myshape_X*7), (myshape_X*7, myshape_X*7), (myshape_X*7, 2)], # 7-7\n",
    "    #[(myshape_X, myshape_X*7), (myshape_X*7, myshape_X*6), (myshape_X*6, 2)], # 7-6\n",
    "    #[(myshape_X, myshape_X*6), (myshape_X*6, myshape_X*6), (myshape_X*6, 2)], # 6-6\n",
    "    #[(myshape_X, myshape_X*6), (myshape_X*6, myshape_X*5), (myshape_X*5, 2)], # 6-5\n",
    "    #[(myshape_X, myshape_X*6), (myshape_X*6, myshape_X*4), (myshape_X*4, 2)], # 6-4\n",
    "    #[(myshape_X, myshape_X*6), (myshape_X*6, myshape_X*3), (myshape_X*3, 2)], # 6-3\n",
    "    #[(myshape_X, myshape_X*5), (myshape_X*5, myshape_X*5), (myshape_X*5, 2)], # 5-5\n",
    "    #[(myshape_X, myshape_X*5), (myshape_X*5, myshape_X*4), (myshape_X*4, 2)], # 5-4\n",
    "    #[(myshape_X, myshape_X*5), (myshape_X*5, myshape_X*3), (myshape_X*3, 2)], # 5-3\n",
    "    #[(myshape_X, myshape_X*4), (myshape_X*4, myshape_X*4), (myshape_X*4, 2)], # 4-4\n",
    "    #[(myshape_X, myshape_X*4), (myshape_X*4, myshape_X*3), (myshape_X*3, 2)],\n",
    "    #[(myshape_X, myshape_X*3), (myshape_X*3, myshape_X*3), (myshape_X*3, 2)],\n",
    "    #[(myshape_X, myshape_X*3), (myshape_X*3, myshape_X*2), (myshape_X*2, 2)],\n",
    "    #[(myshape_X, myshape_X*2), (myshape_X*2, myshape_X*2), (myshape_X*2, 2)],\n",
    "    #[(myshape_X, myshape_X*2), (myshape_X*2, myshape_X), (myshape_X, 2)],\n",
    "    #[(myshape_X, myshape_X), (myshape_X, myshape_X), (myshape_X, 2)],\n",
    "    # 3 hidden layers\n",
    "    #[(myshape_X, myshape_X*6), (myshape_X*6, myshape_X*6), (myshape_X*6, myshape_X*5), (myshape_X*5, 2)], # 6-6-5\n",
    "    #[(myshape_X, myshape_X*6), (myshape_X*6, myshape_X*5), (myshape_X*5, myshape_X*5), (myshape_X*5, 2)], # 6-5-5\n",
    "    #[(myshape_X, myshape_X*6), (myshape_X*6, myshape_X*5), (myshape_X*5, myshape_X*4), (myshape_X*4, 2)], # 6-5-4\n",
    "    #[(myshape_X, myshape_X*6), (myshape_X*6, myshape_X*5), (myshape_X*5, myshape_X*3), (myshape_X*3, 2)], # 6-5-3\n",
    "    #[(myshape_X, myshape_X*6), (myshape_X*6, myshape_X*4), (myshape_X*4, myshape_X*4), (myshape_X*4, 2)], # 6-4-4\n",
    "    #[(myshape_X, myshape_X*6), (myshape_X*6, myshape_X*4), (myshape_X*4, myshape_X*3), (myshape_X*3, 2)], # 6-4-3\n",
    "    #[(myshape_X, myshape_X*6), (myshape_X*6, myshape_X*4), (myshape_X*4, myshape_X*2), (myshape_X*2, 2)], # 6-4-2\n",
    "    #[(myshape_X, myshape_X*6), (myshape_X*6, myshape_X*4), (myshape_X*4, myshape_X*1), (myshape_X*1, 2)], # 6-4-1\n",
    "    #[(myshape_X, myshape_X*6), (myshape_X*6, myshape_X*3), (myshape_X*3, myshape_X*3), (myshape_X*3, 2)], # 6-3-3\n",
    "    #[(myshape_X, myshape_X*5), (myshape_X*5, myshape_X*4), (myshape_X*4, myshape_X*4), (myshape_X*4, 2)], # 5-4-4\n",
    "    #[(myshape_X, myshape_X*5), (myshape_X*5, myshape_X*4), (myshape_X*4, myshape_X*3), (myshape_X*3, 2)], # 5-4-3\n",
    "    #[(myshape_X, myshape_X*5), (myshape_X*5, myshape_X*4), (myshape_X*4, myshape_X*2), (myshape_X*2, 2)], # 5-4-2\n",
    "]\n",
    "\n",
    "#lr_vals = [1e-2, 5e-3, 1e-3, 5e-4]\n",
    "lr_vals = [1e-2, 5e-3]\n",
    "hyperparam_list = list(product(layers, lr_vals))\n",
    "# v1: There are 42'981 total points / 570 positive (1.33%) -> \"natural\" my_weight[1] = (42981-570)/570 = 74.4\n",
    "# v3: There are 41'600 total points / 535 positive (1.29%) -> (41600-535)/535 = 76.8\n",
    "my_weight = torch.Tensor(weight_hyperparam)\n",
    "my_loss = torch.nn.CrossEntropyLoss(weight = my_weight).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340c2c96",
   "metadata": {},
   "source": [
    "## Training and validating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49180b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CV_model(activ_fun, working_dir, F1_score_file, val_loss_file):\n",
    "    \"\"\"\n",
    "    This function runs a cross-validation procedure for each combination of layers + learning rates\n",
    "    Results are saved in a .csv file inside {working_dir}\n",
    "    \"\"\"\n",
    "    # LSTM changes the configuration of the first layer. Thus, need to increase the ...\n",
    "    # size of the 1st MLP layer to lstm_size\n",
    "    if lstm_size:\n",
    "        for cur_hp in hyperparam_list:\n",
    "            cur_hp[0][0] = (lstm_size, cur_hp[0][0][1])\n",
    "    # Recording the validation F1 scores and losses\n",
    "    try:\n",
    "        final_val_F1 = pd.read_csv(f'{working_dir}/{F1_score_file}', index_col = 0)\n",
    "    except FileNotFoundError:\n",
    "        final_val_F1 = pd.DataFrame(np.nan, index = lr_vals, columns = [str(elem) for elem in layers])\n",
    "    try: # Separate try-except to ensure F1 records aren't overwritten if they exist w/o val-loss records\n",
    "        final_val_loss = pd.read_csv(f'{working_dir}/{val_loss_file}', index_col = 0)\n",
    "    except FileNotFoundError:\n",
    "        final_val_loss = pd.DataFrame(np.nan, index = lr_vals, columns = [str(elem) for elem in layers])\n",
    "\n",
    "    # Train and validate\n",
    "    print(f'Beginning CV on activation function {activ_fun} (weight = {weight_hyperparam[1]})')\n",
    "    for cur_idx, cur_hp in enumerate(hyperparam_list):\n",
    "        # We added a new layer configuration to the hyperparameters\n",
    "        if not str(cur_hp[0]) in list(final_val_F1.columns):\n",
    "            final_val_F1.insert(layers.index(cur_hp[0]), str(cur_hp[0]), np.nan) # layers.index to ensure consistent order\n",
    "        if not str(cur_hp[0]) in list(final_val_loss.columns):\n",
    "            final_val_loss.insert(layers.index(cur_hp[0]), str(cur_hp[0]), np.nan)\n",
    "        # We added a new learning rate to the hyperparameters\n",
    "        if not cur_hp[1] in final_val_F1.index.to_list():\n",
    "            final_val_F1.loc[cur_hp[1], :] = np.nan\n",
    "        if not cur_hp[1] in final_val_loss.index.to_list():\n",
    "            final_val_loss.loc[cur_hp[1], :] = np.nan\n",
    "\n",
    "        # Run CV only if we do not have validation losses for this set of parameters\n",
    "        if np.isnan( final_val_F1.at[cur_hp[1], str(cur_hp[0])] ):\n",
    "            print(f'Beginning hyperparameters {cur_idx+1:2}/{len(hyperparam_list)} for {activ_fun}; layers = {cur_hp[0]}, lr = {cur_hp[1]}')\n",
    "            temp_val_F1 = 0\n",
    "            temp_val_loss = 0\n",
    "            my_kfold = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 123)\n",
    "            for fold_idx, (train_idx, val_idx) in enumerate(my_kfold.split(cv_data[:, :-1], cv_data[:, -1])):\n",
    "                print(f'Current fold: {fold_idx+1}/{my_kfold.n_splits}', end = '\\r')\n",
    "                # Creating the Datasets\n",
    "                if lstm_size:\n",
    "                    train_dataset_fold = MyDataset(cv_data[train_idx], cv_lstm_data[train_idx])\n",
    "                    val_dataset_fold = MyDataset(cv_data[val_idx], cv_lstm_data[val_idx])\n",
    "                else:\n",
    "                    train_dataset_fold = MyDataset(cv_data[train_idx])\n",
    "                    val_dataset_fold = MyDataset(cv_data[val_idx])\n",
    "                \n",
    "                num_repeats = 0\n",
    "                while not num_repeats or CM[1,1]+CM[0,1] == 0: # 1st run or pre was = 0 --> fold had a bad initialization; rerun\n",
    "                    if 'delta_t' in locals(): # To ensure some print messages actually print\n",
    "                        del delta_t\n",
    "                    # Creating the DataLoaders\n",
    "                    train_loader_fold = DataLoader(train_dataset_fold, batch_size, shuffle = True)\n",
    "                    val_loader_fold = DataLoader(val_dataset_fold, batch_size, shuffle = True)\n",
    "\n",
    "                    # Declaring the model and optimizer\n",
    "                    model = SequenceMLP(cur_hp[0], activ_fun, lstm_size).cuda()\n",
    "                    optimizer = torch.optim.Adam(model.parameters(), lr = cur_hp[1])\n",
    "                    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor = 0.5, patience = 7, verbose = False, min_lr = 1e-5)\n",
    "                    n_epochs = 65\n",
    "                    # Train and validate\n",
    "                    for epoch in range(n_epochs):\n",
    "                        t1 = time()\n",
    "                        if 'delta_t' in locals():\n",
    "                            print(f'Current fold: {fold_idx+1}/{my_kfold.n_splits}; epoch: {epoch+1:3}/{n_epochs}; number of repeats: {num_repeats:2}; Epoch time = {delta_t:.2f}  ', end = '\\r')\n",
    "                        else:\n",
    "                            print(f'Current fold: {fold_idx+1}/{my_kfold.n_splits}; epoch: {epoch+1:3}/{n_epochs}; number of repeats: {num_repeats:2}')\n",
    "                        train_loss = loop_model(model, optimizer, train_loader_fold, epoch)\n",
    "                        val_loss = loop_model(model, optimizer, val_loader_fold, epoch, evaluation = True)\n",
    "                        scheduler.step(val_loss)\n",
    "                        t2 = time()\n",
    "                        delta_t = t2-t1\n",
    "                    # Calculating and recording the validation F1 score for this fold\n",
    "                    val_pred = torch.empty((len(val_loader_fold.dataset), 2))\n",
    "                    val_y = torch.empty((len(val_loader_fold.dataset)), dtype = torch.long)\n",
    "                    for idx, data in enumerate(val_loader_fold):\n",
    "                        if lstm_size:\n",
    "                            X, y, lstm = data\n",
    "                            lstm = lstm.cuda()\n",
    "                        else:\n",
    "                            X, y = data\n",
    "                            lstm = None\n",
    "                        X = X.cuda()\n",
    "                        pred = model(X, lstm).cpu().detach()\n",
    "                        val_pred[idx*batch_size:(idx*batch_size)+len(pred), :] = pred\n",
    "                        val_y[idx*batch_size:(idx*batch_size)+len(y)] = y\n",
    "                    val_pred_CM = val_pred.argmax(axis=1)\n",
    "                    CM = confusion_matrix(val_y, val_pred_CM) # Confusion matrix to make F1 calcs easier\n",
    "                    num_repeats += 1\n",
    "                rec = CM[1,1]/(CM[1,1]+CM[1,0])\n",
    "                pre = CM[1,1]/(CM[1,1]+CM[0,1])\n",
    "                if rec and pre: # Avoids dividing by 0 when calculating F1\n",
    "                    F1 = 2/(1/rec + 1/pre)\n",
    "                else:\n",
    "                    F1 = 0\n",
    "                temp_val_F1 += F1 / my_kfold.n_splits\n",
    "                temp_val_loss += my_loss(val_pred.cuda(), val_y.cuda()) / my_kfold.n_splits\n",
    "\n",
    "            # Saving the average validation F1 after CV\n",
    "            temp_val_loss = temp_val_loss.cpu().detach().item()\n",
    "            final_val_F1.at[cur_hp[1], str(cur_hp[0])] = temp_val_F1\n",
    "            final_val_loss.at[cur_hp[1], str(cur_hp[0])] = temp_val_loss\n",
    "            final_val_F1.to_csv(f'{working_dir}/{F1_score_file}')\n",
    "            final_val_loss.to_csv(f'{working_dir}/{val_loss_file}')\n",
    "    return final_val_F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a39504e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "final_val_F1_list = np.empty_like(activ_fun_list, dtype = object) # This will hold multiple DataFrames, one for each activation fun type\n",
    "for idx, activ_fun in enumerate(activ_fun_list):\n",
    "    # Setup for the results files\n",
    "    F1_score_file = f'ANN_F1_{activ_fun}_{weight_hyperparam[1]}weight.csv'\n",
    "    val_loss_file = f'ANN_val-loss_{activ_fun}_{weight_hyperparam[1]}weight.csv'\n",
    "    # Running the CV (no nested validation) / groups of CVs (nested validation)\n",
    "    final_val_F1_list[idx] = CV_model(activ_fun, working_dir, F1_score_file, val_loss_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27a8d0d",
   "metadata": {},
   "source": [
    "## Final Evaluation - Testing the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1ebb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_final_evaluation(model, activ_fun, threshold = 0.5):\n",
    "    model.eval()\n",
    "\n",
    "    # Train loss\n",
    "    train_pred = torch.empty((len(train_loader.dataset), 2))\n",
    "    train_y = torch.empty((len(train_loader.dataset)), dtype = torch.long)\n",
    "    for idx, data in enumerate(train_loader):\n",
    "        if lstm_size:\n",
    "            X, y, lstm = data\n",
    "            lstm = lstm.cuda()\n",
    "        else:\n",
    "            X, y = data\n",
    "            lstm = None\n",
    "        X = X.cuda()\n",
    "        pred = model(X, lstm).cpu().detach()\n",
    "        train_pred[idx*batch_size:(idx*batch_size)+len(pred), :] = pred\n",
    "        train_y[idx*batch_size:(idx*batch_size)+len(y)] = y\n",
    "    #train_loss = my_loss(train_pred, train_y)\n",
    "    #print(f'The train loss was {train_loss.item():.3f}')\n",
    "    # Renormalizing the train_pred\n",
    "    train_pred = (train_pred.T / train_pred.sum(axis=1)).T\n",
    "    # Train confusion matrix\n",
    "    train_pred_CM = train_pred[:, 1] >= threshold\n",
    "    CM = confusion_matrix(train_y, train_pred_CM)\n",
    "    if CM[1,1]+CM[0,1]:\n",
    "        rec = CM[1,1]/(CM[1,1]+CM[1,0])\n",
    "        pre = CM[1,1]/(CM[1,1]+CM[0,1])\n",
    "        f1 = 2/(1/rec + 1/pre)\n",
    "    else:\n",
    "        rec, pre, f1 = 0, 0, 0\n",
    "    print(f'The train recall was {rec*100:.2f}%')\n",
    "    print(f'The train precision was {pre*100:.2f}%')\n",
    "    print(f'The train F1 score was {f1*100:.2f}%')\n",
    "    #fig, ax = plt.subplots(figsize = (8,8))\n",
    "    #ax.set_title(f'{activ_fun} - Train Confusion Matrix')\n",
    "    #_ = ConfusionMatrixDisplay(CM).plot(ax = ax)\n",
    "    \n",
    "    # Test loss\n",
    "    test_pred = torch.empty((len(test_loader.dataset), 2))\n",
    "    test_y = torch.empty((len(test_loader.dataset)), dtype = torch.long)\n",
    "    for idx, data in enumerate(test_loader):\n",
    "        if lstm_size:\n",
    "            X, y, lstm = data\n",
    "            lstm = lstm.cuda()\n",
    "        else:\n",
    "            X, y = data\n",
    "            lstm = None\n",
    "        X = X.cuda()\n",
    "        pred = model(X, lstm).cpu().detach()\n",
    "        test_pred[idx*batch_size:(idx*batch_size)+len(pred), :] = pred\n",
    "        test_y[idx*batch_size:(idx*batch_size)+len(y)] = y\n",
    "    test_loss = my_loss(test_pred.cuda(), test_y.cuda())\n",
    "    print(f'The test loss was {test_loss:.3f}')\n",
    "    # Renormalizing the test_pred\n",
    "    test_pred = (test_pred.T / test_pred.sum(axis=1)).T\n",
    "    # Test confusion matrix\n",
    "    test_pred_CM = test_pred[:, 1] >= threshold\n",
    "    CM = confusion_matrix(test_y, test_pred_CM)\n",
    "    if CM[1,1]+CM[0,1]:\n",
    "        rec = CM[1,1]/(CM[1,1]+CM[1,0])\n",
    "        pre = CM[1,1]/(CM[1,1]+CM[0,1])\n",
    "        f1 = 2/(1/rec + 1/pre)\n",
    "    else:\n",
    "        rec, pre, f1 = 0, 0, 0\n",
    "    print(f'The test recall was {rec*100:.2f}%')\n",
    "    print(f'The test precision was {pre*100:.2f}%')\n",
    "    print(f'The test F1 score was {f1*100:.2f}%')\n",
    "    fig, ax = plt.subplots(figsize = (8,8))\n",
    "    ax.set_title(f'{activ_fun} - Test Confusion Matrix')\n",
    "    _ = ConfusionMatrixDisplay(CM).plot(ax = ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d768af3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Creating the full training Dataset / DataLoader\n",
    "if lstm_size:\n",
    "    train_dataset = MyDataset(cv_data, cv_lstm_data)\n",
    "    test_dataset = MyDataset(test_data, test_lstm_data)\n",
    "else:\n",
    "    train_dataset = MyDataset(cv_data)\n",
    "    test_dataset = MyDataset(test_data)\n",
    "# Creating the DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle = True)\n",
    "test_loader = DataLoader(test_dataset, batch_size, shuffle = True)\n",
    "\n",
    "for final_val_F1, activ_fun in zip(final_val_F1_list, activ_fun_list):\n",
    "    best_model_file = f'ANN_{activ_fun}_{weight_hyperparam[1]}weight_dict.pt'\n",
    "    # Finding the best hyperparameters\n",
    "    best_idx = np.unravel_index(np.nanargmax(final_val_F1.values), final_val_F1.shape)\n",
    "    best_LR = final_val_F1.index[best_idx[0]]\n",
    "    best_neurons_str = final_val_F1.columns[best_idx[1]]\n",
    "    # Converting the best number of neurons from str to list\n",
    "    best_neurons = []\n",
    "    temp_number = []\n",
    "    temp_tuple = []\n",
    "    for elem in best_neurons_str:\n",
    "        if elem in '0123456789':\n",
    "            temp_number.append(elem)\n",
    "        elif elem in {',', ')'} and temp_number: # Finished a number. 2nd check because there is a comma right after )\n",
    "            converted_number = ''.join(temp_number)\n",
    "            temp_tuple.append( int(converted_number) )\n",
    "            temp_number = []\n",
    "        if elem in {')'}: # Also finished a tuple\n",
    "            best_neurons.append(tuple(temp_tuple))\n",
    "            temp_tuple = []\n",
    "    # Re-declaring the model\n",
    "    model = SequenceMLP(best_neurons, activ_fun, lstm_size).cuda()\n",
    "\n",
    "    # Checking if we already retrained this model\n",
    "    try:\n",
    "        mydict = torch.load(f'{working_dir}/{best_model_file}')\n",
    "        model.load_state_dict(mydict)\n",
    "    except FileNotFoundError: # Retraining the model with the full training set\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = best_LR)\n",
    "        # Retrain\n",
    "        for epoch in range(n_epochs):\n",
    "            if lstm_size:\n",
    "                print(f'For {activ_fun}: epoch {epoch+1:3}/{n_epochs}', end = '\\r')\n",
    "            train_loss = loop_model(model, optimizer, train_loader, epoch)\n",
    "        # Save the retrained model\n",
    "        torch.save(model.state_dict(), f'{working_dir}/{best_model_file}')\n",
    "    \n",
    "    # CV Data\n",
    "    print(f'Final results for {activ_fun} & weight {weight_hyperparam[1]}')\n",
    "    print(f'Best hyperparameters: {best_neurons}, {best_LR}')\n",
    "    print(f'CV F1 score: {final_val_F1.iat[best_idx]:.4f}')\n",
    "    run_final_evaluation(model, activ_fun, 0.5)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
